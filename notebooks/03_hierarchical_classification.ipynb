{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical 3D Medical Image Classification\n",
    "\n",
    "This notebook trains a hierarchical classification model on **all 5 merged 3D MedMNIST datasets**:\n",
    "- **OrganMNIST3D** (11 classes, multi-region)\n",
    "- **NoduleMNIST3D** (2 classes, chest)\n",
    "- **AdrenalMNIST3D** (2 classes, abdomen)\n",
    "- **FractureMNIST3D** (3 classes, chest)\n",
    "- **VesselMNIST3D** (2 classes, brain)\n",
    "\n",
    "## Training Pipeline\n",
    "1. **Stage 1 (Coarse)**: Classify anatomical region (abdomen, chest, brain)\n",
    "2. **Stage 2 (Fine)**: Classify pathology within each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA GeForce RTX 3070\n",
      "GPU memory: 8.21 GB\n",
      "Platform: NVIDIA CUDA\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import (\n",
    "    DEVICE, DATA_CONFIG, MODEL_CONFIG, TRAINING_CONFIG,\n",
    "    PATHS, set_seed, DEFAULT_MERGED_DATASETS\n",
    ")\n",
    "from utils.data_loader import create_hierarchical_dataset\n",
    "from utils.hierarchical_model import HierarchicalClassificationModel\n",
    "from utils.trainer import HierarchicalTrainer\n",
    "from utils.visualization import plot_training_history\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets: ['organ', 'nodule', 'adrenal', 'fracture', 'vessel']\n",
      "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
      "Using downloaded and verified file: /home/luca/.medmnist/nodulemnist3d.npz\n",
      "Using downloaded and verified file: /home/luca/.medmnist/adrenalmnist3d.npz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load all 5 merged datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEFAULT_MERGED_DATASETS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_loader, val_loader, test_loader, dataset_info \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_hierarchical_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets_to_include\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_MERGED_DATASETS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMERGED DATASET INFO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/HierarchicalMIC/notebooks/../utils/data_loader.py:402\u001b[0m, in \u001b[0;36mcreate_hierarchical_dataset\u001b[0;34m(datasets_to_include, batch_size, num_workers)\u001b[0m\n\u001b[1;32m    398\u001b[0m     datasets_to_include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morgan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madrenal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfracture\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvessel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    400\u001b[0m datasets_config \u001b[38;5;241m=\u001b[39m {name: \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m datasets_to_include}\n\u001b[0;32m--> 402\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHierarchicalMedMNISTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m HierarchicalMedMNISTDataset(datasets_config, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    404\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m HierarchicalMedMNISTDataset(datasets_config, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/HierarchicalMIC/notebooks/../utils/data_loader.py:339\u001b[0m, in \u001b[0;36mHierarchicalMedMNISTDataset.__init__\u001b[0;34m(self, datasets_config, split, augment, augmentation_config)\u001b[0m\n\u001b[1;32m    336\u001b[0m fine_to_coarse_map \u001b[38;5;241m=\u001b[39m FINE_TO_COARSE_MAPPING[dataset_name]\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m--> 339\u001b[0m     img, label \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# Get fine label (handle both scalar and array labels)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     fine_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(label\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(label, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqueeze\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(label)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/medmnist/dataset.py:198\u001b[0m, in \u001b[0;36mMedMNIST3D.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03mreturn: (without transform/target_transofrm)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    img: an array of 1x28x28x28 or 3x28x28x28 (if `as_RGB=True`), in [0,1]\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    target: np.array of `L` (L=1 for single-label)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_rgb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/numpy/core/shape_base.py:362\u001b[0m, in \u001b[0;36m_stack_dispatcher\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stack_dispatcher\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    363\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    364\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m _arrays_for_stack_dispatcher(arrays)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# optimize for the typical case where only arrays is provided\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load all 5 merged datasets\n",
    "print(f\"Loading datasets: {DEFAULT_MERGED_DATASETS}\")\n",
    "\n",
    "train_loader, val_loader, test_loader, dataset_info = create_hierarchical_dataset(\n",
    "    datasets_to_include=DEFAULT_MERGED_DATASETS,\n",
    "    batch_size=DATA_CONFIG['batch_size'],\n",
    "    num_workers=DATA_CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGED DATASET INFO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Datasets: {dataset_info['datasets_included']}\")\n",
    "print(f\"Train samples: {dataset_info['train_samples']:,}\")\n",
    "print(f\"Val samples: {dataset_info['val_samples']:,}\")\n",
    "print(f\"Test samples: {dataset_info['test_samples']:,}\")\n",
    "print(f\"\\nCoarse classes (regions): {dataset_info['num_coarse_classes']}\")\n",
    "print(f\"Region mapping: {dataset_info['idx_to_region']}\")\n",
    "print(f\"Fine classes: {dataset_info['num_fine_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data format\n",
    "print(\"\\nVerifying data format...\")\n",
    "for imgs, coarse_labels, fine_labels in train_loader:\n",
    "    print(f\"  Image shape: {imgs.shape}\")\n",
    "    print(f\"  Coarse labels: {coarse_labels.shape} - unique: {coarse_labels.unique().tolist()}\")\n",
    "    print(f\"  Fine labels: {fine_labels.shape} - unique: {fine_labels.unique().tolist()[:10]}...\")\n",
    "    break\n",
    "print(\" Data format verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Hierarchical Model\n",
    "\n",
    "The model consists of:\n",
    "- **Coarse Classifier**: Shared 3D CNN backbone + region classification head\n",
    "- **Fine Classifiers**: Separate classifier for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure region-specific classes\n",
    "# Each region has a different number of fine-grained classes\n",
    "region_configs = {\n",
    "    'abdomen': 2,   # AdrenalMNIST3D classes\n",
    "    'brain': 2,     # VesselMNIST3D classes  \n",
    "    'chest': 5,     # NoduleMNIST3D (2) + FractureMNIST3D (3)\n",
    "    'multi': 11,    # OrganMNIST3D classes\n",
    "}\n",
    "\n",
    "# Get region index mapping from dataset\n",
    "region_idx_to_name = dataset_info['idx_to_region']\n",
    "\n",
    "print(\"Region configurations:\")\n",
    "for region, num_classes in region_configs.items():\n",
    "    print(f\"  {region}: {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical model\n",
    "model = HierarchicalClassificationModel(\n",
    "    region_configs=region_configs,\n",
    "    architecture=MODEL_CONFIG['architecture'],\n",
    "    coarse_model_type=MODEL_CONFIG['coarse_architecture'],\n",
    "    fine_model_type=MODEL_CONFIG['fine_architecture'],\n",
    "    dropout_rate=MODEL_CONFIG['dropout_rate'],\n",
    "    region_idx_to_name=region_idx_to_name,\n",
    "    num_total_organs=dataset_info['num_fine_classes'],\n",
    "    use_subtypes=False\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Coarse architecture: {MODEL_CONFIG['coarse_architecture']}\")\n",
    "print(f\"  Fine architecture: {MODEL_CONFIG['fine_architecture']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Hierarchical Model\n",
    "\n",
    "Training proceeds in two stages:\n",
    "1. **Stage 1**: Train coarse classifier to predict anatomical regions\n",
    "2. **Stage 2**: Freeze coarse classifier, train region-specific fine classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = HierarchicalTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    coarse_weight=0.3,\n",
    "    fine_weight=0.7\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Coarse epochs: {TRAINING_CONFIG['coarse_epochs']}\")\n",
    "print(f\"  Fine epochs: {TRAINING_CONFIG['fine_epochs']}\")\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    coarse_epochs=TRAINING_CONFIG['coarse_epochs'],\n",
    "    fine_epochs=TRAINING_CONFIG['fine_epochs']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Coarse training\n",
    "if history['coarse_train_loss']:\n",
    "    axes[0, 0].plot(history['coarse_train_loss'], label='Coarse Train Loss', color='blue')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Stage 1: Coarse Classifier Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if history['coarse_train_acc']:\n",
    "    axes[0, 1].plot(history['coarse_train_acc'], label='Coarse Train Acc', color='blue')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Stage 1: Coarse Classifier Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Fine training\n",
    "if history['fine_train_loss']:\n",
    "    axes[1, 0].plot(history['fine_train_loss'], label='Fine Train Loss', color='green')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Stage 2: Fine Classifier Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if history['fine_train_acc']:\n",
    "    axes[1, 1].plot(history['fine_train_acc'], label='Fine Train Acc', color='green')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].set_title('Stage 2: Fine Classifier Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PATHS['figures']}/hierarchical_training_{MODEL_CONFIG['architecture']}.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hierarchical_model(model, test_loader, device, region_idx_to_name):\n",
    "    \"\"\"Evaluate hierarchical model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    coarse_correct = 0\n",
    "    coarse_total = 0\n",
    "    \n",
    "    fine_correct = {name: 0 for name in region_idx_to_name.values()}\n",
    "    fine_total = {name: 0 for name in region_idx_to_name.values()}\n",
    "    \n",
    "    all_coarse_preds = []\n",
    "    all_coarse_labels = []\n",
    "    all_fine_preds = []\n",
    "    all_fine_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, coarse_labels, fine_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(device, dtype=torch.float32)\n",
    "            if imgs.max() > 1:\n",
    "                imgs = imgs / 255.0\n",
    "            \n",
    "            coarse_labels = coarse_labels.long().to(device)\n",
    "            fine_labels = fine_labels.long().to(device)\n",
    "            \n",
    "            # Stage 1: Coarse prediction\n",
    "            coarse_logits = model.forward_coarse(imgs)\n",
    "            coarse_preds = coarse_logits.argmax(1)\n",
    "            \n",
    "            coarse_correct += (coarse_preds == coarse_labels).sum().item()\n",
    "            coarse_total += imgs.size(0)\n",
    "            \n",
    "            all_coarse_preds.extend(coarse_preds.cpu().numpy())\n",
    "            all_coarse_labels.extend(coarse_labels.cpu().numpy())\n",
    "            \n",
    "            # Stage 2: Fine prediction per region\n",
    "            for region_idx, region_name in region_idx_to_name.items():\n",
    "                mask = (coarse_labels == region_idx)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                \n",
    "                region_imgs = imgs[mask]\n",
    "                region_fine_labels = fine_labels[mask]\n",
    "                \n",
    "                fine_logits = model.forward_fine(region_imgs, region_name)\n",
    "                fine_preds = fine_logits.argmax(1)\n",
    "                \n",
    "                fine_correct[region_name] += (fine_preds == region_fine_labels).sum().item()\n",
    "                fine_total[region_name] += region_imgs.size(0)\n",
    "                \n",
    "                all_fine_preds.extend(fine_preds.cpu().numpy())\n",
    "                all_fine_labels.extend(region_fine_labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    coarse_acc = coarse_correct / coarse_total if coarse_total > 0 else 0\n",
    "    \n",
    "    fine_acc_per_region = {}\n",
    "    for region_name in region_idx_to_name.values():\n",
    "        if fine_total[region_name] > 0:\n",
    "            fine_acc_per_region[region_name] = fine_correct[region_name] / fine_total[region_name]\n",
    "        else:\n",
    "            fine_acc_per_region[region_name] = 0.0\n",
    "    \n",
    "    overall_fine_acc = sum(fine_correct.values()) / sum(fine_total.values()) if sum(fine_total.values()) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'coarse_accuracy': coarse_acc,\n",
    "        'fine_accuracy_per_region': fine_acc_per_region,\n",
    "        'overall_fine_accuracy': overall_fine_acc,\n",
    "        'coarse_predictions': np.array(all_coarse_preds),\n",
    "        'coarse_labels': np.array(all_coarse_labels),\n",
    "        'fine_predictions': np.array(all_fine_preds),\n",
    "        'fine_labels': np.array(all_fine_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = evaluate_hierarchical_model(model, test_loader, DEVICE, region_idx_to_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStage 1 (Coarse) Accuracy: {results['coarse_accuracy']:.4f}\")\n",
    "print(f\"\\nStage 2 (Fine) Accuracy per Region:\")\n",
    "for region, acc in results['fine_accuracy_per_region'].items():\n",
    "    print(f\"  {region}: {acc:.4f}\")\n",
    "print(f\"\\nOverall Fine Accuracy: {results['overall_fine_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(PATHS['models'], exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = f\"{PATHS['models']}/hierarchical_{MODEL_CONFIG['architecture']}.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'region_configs': region_configs,\n",
    "    'dataset_info': dataset_info,\n",
    "    'history': history,\n",
    "    'test_results': results,\n",
    "    'config': {\n",
    "        'coarse_architecture': MODEL_CONFIG['coarse_architecture'],\n",
    "        'fine_architecture': MODEL_CONFIG['fine_architecture'],\n",
    "        'dropout_rate': MODEL_CONFIG['dropout_rate'],\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"\\n Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDatasets used: {', '.join(dataset_info['datasets_included'])}\")\n",
    "print(f\"Total training samples: {dataset_info['train_samples']:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Coarse: {MODEL_CONFIG['coarse_architecture']}\")\n",
    "print(f\"  Fine: {MODEL_CONFIG['fine_architecture']}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Coarse accuracy: {results['coarse_accuracy']:.4f}\")\n",
    "print(f\"  Fine accuracy: {results['overall_fine_accuracy']:.4f}\")\n",
    "print(f\"\\nModel saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
