{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15da483",
   "metadata": {},
   "source": [
    "This notebook compares different 3D CNN architectures for medical image classification:\n",
    "\n",
    "ResNet-18 (3D) - Recommended default (~33M params)\n",
    "ResNet-34 (3D) - More capacity (~63M params)\n",
    "ResNet-50 (3D) - Maximum performance (~46M params)\n",
    "DenseNet-121 (3D) - Efficient feature reuse (~5.6M params)\n",
    "EfficientNet-B0 (3D) - Most parameter efficient (~1.2M params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975f3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA GeForce RTX 3070\n",
      "GPU memory: 8.21 GB\n",
      "Platform: NVIDIA CUDA\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.cnn_3d_models import get_3d_model\n",
    "from utils.data_loader import get_medmnist_dataloaders\n",
    "from utils.metrics import evaluate_model\n",
    "from config import *\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33d4171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
      "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
      "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
      "Test set loaded: 610 samples\n",
      "Number of classes: 11\n",
      "Output directories ready\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "_, _, test_loader, num_classes = get_medmnist_dataloaders(\n",
    "    dataset_name='organ',\n",
    "    batch_size=32,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Test set loaded: {len(test_loader.dataset)} samples\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create output directories\n",
    "Path('../results').mkdir(exist_ok=True)\n",
    "Path('../figures').mkdir(exist_ok=True)\n",
    "print(\"Output directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fbc440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architectures to compare:\n",
      "  - ResNet-18            (resnet18_3d)\n",
      "  - ResNet-34            (resnet34_3d)\n",
      "  - ResNet-50            (resnet50_3d)\n",
      "  - DenseNet-121         (densenet121_3d)\n",
      "  - EfficientNet-B0      (efficientnet3d_b0)\n"
     ]
    }
   ],
   "source": [
    "# Architectures to compare\n",
    "ARCHITECTURES = {\n",
    "    'ResNet-18': 'resnet18_3d',\n",
    "    'ResNet-34': 'resnet34_3d',\n",
    "    'ResNet-50': 'resnet50_3d',\n",
    "    'DenseNet-121': 'densenet121_3d',\n",
    "    'EfficientNet-B0': 'efficientnet3d_b0'\n",
    "}\n",
    "\n",
    "print(\"Architectures to compare:\")\n",
    "for name, arch in ARCHITECTURES.items():\n",
    "    print(f\"  - {name:20s} ({arch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f2b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model_by_arch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_baseline.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_by_arch\u001b[49m(arch, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      9\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Try to load checkpoint\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model_by_arch' is not defined"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "model_info = {}\n",
    "\n",
    "for name, arch in ARCHITECTURES.items():\n",
    "    checkpoint_path = Path(f'../models/{arch}_baseline.pth')\n",
    "    \n",
    "    # Create model\n",
    "    model = get_3d_model(arch, num_classes=num_classes).to(DEVICE)\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Try to load checkpoint\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            \n",
    "            models[name] = model\n",
    "            model_info[name] = {\n",
    "                'architecture': arch,\n",
    "                'params': params,\n",
    "                'trained': True,\n",
    "                'val_acc': max(checkpoint.get('history', {}).get('val_acc', [0.0]))\n",
    "            }\n",
    "            print(f\"{name:20s} - Loaded ({params/1e6:.1f}M params)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name:20s} - Error: {e}\")\n",
    "    else:\n",
    "        model.eval()\n",
    "        models[name] = model\n",
    "        model_info[name] = {\n",
    "            'architecture': arch,\n",
    "            'params': params,\n",
    "            'trained': False,\n",
    "            'val_acc': 0.0\n",
    "        }\n",
    "        print(f\"{name:20s} - Not trained ({params/1e6:.1f}M params)\")\n",
    "\n",
    "trained_count = sum(1 for info in model_info.values() if info['trained'])\n",
    "print(f\"\\n Loaded {len(models)} models ({trained_count} trained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "for name, info in model_info.items():\n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Architecture': info['architecture'],\n",
    "        'Parameters (M)': info['params'] / 1e6,\n",
    "        'Trained': '✓' if info['trained'] else '✗',\n",
    "        'Val Accuracy': f\"{info['val_acc']:.4f}\" if info['trained'] else 'N/A'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "if trained_count == 0:\n",
    "    print(\"\\n WARNING: No trained models found!\")\n",
    "    print(\"   Run notebooks 02 and 03 first to train models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if not model_info[name]['trained']:\n",
    "        print(f\"Skipping {name} (not trained)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    metrics, preds, labels = evaluate_model(model, test_loader, DEVICE)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1_score': metrics['f1_score'],\n",
    "        'confusion_matrix': metrics['confusion_matrix'],\n",
    "        'per_class': metrics['per_class']\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n Evaluated {len(results)} model(s)\")\n",
    "else:\n",
    "    print(\"\\n No trained models to evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for name in results.keys():\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Architecture': model_info[name]['architecture'],\n",
    "            'Params (M)': model_info[name]['params'] / 1e6,\n",
    "            'Accuracy': results[name]['accuracy'],\n",
    "            'Precision': results[name]['precision'],\n",
    "            'Recall': results[name]['recall'],\n",
    "            'F1-Score': results[name]['f1_score']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Save\n",
    "    comparison_df.to_csv('../results/architecture_comparison.csv', index=False)\n",
    "    print(\"\\n Saved to '../results/architecture_comparison.csv'\")\n",
    "else:\n",
    "    print(\"No results to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if results:\n",
    "    # Performance bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    x = np.arange(len(results))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        values = [results[name][metric.lower().replace('-', '_')] for name in results.keys()]\n",
    "        offset = (i - 1.5) * width\n",
    "        bars = ax.bar(x + offset, values, width, label=metric, alpha=0.8)\n",
    "        \n",
    "        # Add labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Architecture Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(results.keys(), rotation=15, ha='right')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Saved to '../figures/architecture_comparison.png'\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage-wise evaluation of hierarchical classifiers\n",
    "from utils.hierarchical_model import HierarchicalClassificationModel\n",
    "from utils.coarse_classifier import CoarseAnatomicalClassifier\n",
    "from utils.fine_classifier import RegionSpecificPathologyNetwork\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Define architectures to evaluate\n",
    "STAGE_ARCHITECTURES = {\n",
    "    'ResNet-18': 'resnet18_3d',\n",
    "    'ResNet-34': 'resnet34_3d',\n",
    "    'ResNet-50': 'resnet50_3d',\n",
    "    'DenseNet-121': 'densenet121_3d',\n",
    "    'EfficientNet-B0': 'efficientnet3d_b0'\n",
    "}\n",
    "# Build organ-to-region mapping\n",
    "organ_to_region_map = {}\n",
    "for organ_idx, organ_name in ORGAN_CLASSES.items():\n",
    "    region_name = ORGAN_TO_REGION[organ_name]\n",
    "    region_idx = list(REGION_CONFIGS.keys()).index(region_name)\n",
    "    organ_to_region_map[organ_idx] = region_idx\n",
    "REGION_IDX_TO_NAME = {i: name for i, name in enumerate(REGION_CONFIGS.keys())}\n",
    "# Prepare test data with region labels\n",
    "all_images = []\n",
    "all_organ_labels = []\n",
    "all_region_labels = []\n",
    "for images, labels in test_loader:\n",
    "    all_images.append(images)\n",
    "    all_organ_labels.append(labels.squeeze())\n",
    "    region_labels = torch.tensor([organ_to_region_map[l.item()] for l in labels.squeeze()])\n",
    "    all_region_labels.append(region_labels)\n",
    "test_images = torch.cat(all_images, dim=0).float()\n",
    "test_organ_labels = torch.cat(all_organ_labels, dim=0)\n",
    "test_region_labels = torch.cat(all_region_labels, dim=0)\n",
    "print(f\"Test set: {len(test_images)} samples\")\n",
    "print(f\"Unique regions: {len(torch.unique(test_region_labels))}\")\n",
    "print(f\"Unique organs: {len(torch.unique(test_organ_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 Evaluation: Coarse anatomical region classification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: COARSE ANATOMICAL REGION CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stage1_results = []\n",
    "\n",
    "for arch_name, arch_type in STAGE_ARCHITECTURES.items():\n",
    "    print(f\"\\nEvaluating {arch_name}...\")\n",
    "    \n",
    "    # Check for trained checkpoint\n",
    "    checkpoint_path = Path(f'../models/hierarchical_model_{arch_type}.pth')\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"  ✗ No trained model found: {checkpoint_path}\")\n",
    "        stage1_results.append({\n",
    "            'Stage': 'Stage 1',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-Score': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "        print(f\"  ✓ Loaded checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        # Get saved configs from checkpoint\n",
    "        saved_region_configs = checkpoint['region_configs']\n",
    "        num_regions = len(saved_region_configs)\n",
    "        saved_region_idx_to_name = {i: name for i, name in enumerate(saved_region_configs.keys())}\n",
    "        \n",
    "        # Create Stage 1 classifier with correct number of regions\n",
    "        coarse_model = CoarseAnatomicalClassifier(\n",
    "            architecture=arch_type,\n",
    "            num_regions=num_regions,  # Use checkpoint's num_regions\n",
    "            dropout_rate=0.3,\n",
    "            region_names=saved_region_idx_to_name\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Load trained weights\n",
    "        coarse_model.load_state_dict(checkpoint['coarse_model_state'])\n",
    "        coarse_model.eval()\n",
    "        \n",
    "        # Rebuild organ-to-region mapping from checkpoint\n",
    "        saved_organ_to_region = checkpoint['organ_to_region_map']\n",
    "        \n",
    "        # Rebuild test region labels using checkpoint's mapping\n",
    "        test_region_labels_fixed = torch.tensor([\n",
    "            saved_organ_to_region[l.item()] for l in test_organ_labels\n",
    "        ])\n",
    "        \n",
    "        # Evaluate in batches\n",
    "        all_preds = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_images), batch_size):\n",
    "                batch = test_images[i:i+batch_size].float().to(DEVICE)\n",
    "                logits = coarse_model(batch)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.append(preds.cpu())\n",
    "        \n",
    "        all_preds = torch.cat(all_preds)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        y_true = test_region_labels_fixed.numpy()\n",
    "        y_pred = all_preds.numpy()\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        stage1_results.append({\n",
    "            'Stage': 'Stage 1',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        del coarse_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        stage1_results.append({\n",
    "            'Stage': 'Stage 1',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-Score': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a62da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 Evaluation: Fine organ classification (using full hierarchical model)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: FINE ORGAN CLASSIFICATION (Full Hierarchical Model)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stage2_results = []\n",
    "\n",
    "for arch_name, arch_type in STAGE_ARCHITECTURES.items():\n",
    "    print(f\"\\nEvaluating {arch_name}...\")\n",
    "    \n",
    "    # Check for trained checkpoint\n",
    "    checkpoint_path = Path(f'../models/hierarchical_model_{arch_type}.pth')\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"  ✗ No trained model found: {checkpoint_path}\")\n",
    "        stage2_results.append({\n",
    "            'Stage': 'Stage 2',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-Score': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        # Get saved configs\n",
    "        saved_region_configs = checkpoint['region_configs']\n",
    "        saved_organ_to_region = checkpoint['organ_to_region_map']\n",
    "        \n",
    "        # Create full hierarchical model\n",
    "        hierarchical_model = HierarchicalClassificationModel(\n",
    "            region_configs=saved_region_configs,\n",
    "            coarse_model_type=arch_type,\n",
    "            fine_model_type=arch_type,\n",
    "            dropout_rate=0.3,\n",
    "            organ_to_region_map=saved_organ_to_region,\n",
    "            num_total_organs=num_classes,\n",
    "            region_idx_to_name=REGION_IDX_TO_NAME\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Load trained weights\n",
    "        hierarchical_model.load_state_dict(checkpoint['hierarchical_model_state'])\n",
    "        hierarchical_model.eval()\n",
    "        \n",
    "        # Evaluate using the standard evaluate_model function\n",
    "        from utils.metrics import evaluate_model\n",
    "        metrics, preds, labels = evaluate_model(hierarchical_model, test_loader, DEVICE)\n",
    "        \n",
    "        acc = metrics['accuracy']\n",
    "        prec = metrics['precision']\n",
    "        rec = metrics['recall']\n",
    "        f1 = metrics['f1_score']\n",
    "        \n",
    "        stage2_results.append({\n",
    "            'Stage': 'Stage 2',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        del hierarchical_model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        stage2_results.append({\n",
    "            'Stage': 'Stage 2',\n",
    "            'Backbone': arch_name,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-Score': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ef629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and display stage-wise results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STAGE-WISE EVALUATION SUMMARY (Table 2)\")\n",
    "print(\"=\"*100)\n",
    "stagewise_df = pd.DataFrame(stage1_results + stage2_results)\n",
    "stagewise_df = stagewise_df[['Stage', 'Backbone', 'Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "# Format numeric columns\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    stagewise_df[col] = stagewise_df[col].apply(lambda x: f\"{x:.4f}\" if x is not None else 'N/A')\n",
    "print(stagewise_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "# Save to CSV\n",
    "stagewise_csv_path = '../results/stagewise_evaluation.csv'\n",
    "stagewise_df.to_csv(stagewise_csv_path, index=False)\n",
    "print(f\"\\n✓ Stage-wise evaluation saved to '{stagewise_csv_path}'\")\n",
    "# Display as formatted table for paper\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"FORMATTED FOR PAPER (Table 2):\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'Stage':<12} {'Backbone':<18} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\"*100)\n",
    "current_stage = None\n",
    "for _, row in stagewise_df.iterrows():\n",
    "    stage_display = row['Stage'] if row['Stage'] != current_stage else ''\n",
    "    current_stage = row['Stage']\n",
    "    print(f\"{stage_display:<12} {row['Backbone']:<18} {row['Accuracy']:<12} {row['Precision']:<12} {row['Recall']:<12} {row['F1-Score']:<12}\")\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Accuracy vs Parameters\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for i, name in enumerate(results.keys()):\n",
    "        params = model_info[name]['params'] / 1e6\n",
    "        acc = results[name]['accuracy']\n",
    "        ax.scatter(params, acc, s=300, alpha=0.7, label=name, edgecolors='black', linewidth=2)\n",
    "        ax.annotate(name, (params, acc), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Accuracy vs Model Size', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/accuracy_vs_parameters.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Saved to '../figures/accuracy_vs_parameters.png'\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8922b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Best model confusion matrix\n",
    "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "    best_result = results[best_name]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    class_names = [ORGAN_CLASSES[i] for i in range(num_classes)]\n",
    "    sns.heatmap(\n",
    "        best_result['confusion_matrix'],\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"Confusion Matrix - {best_name}\\nAccuracy: {best_result['accuracy']:.4f}\",\n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Confusion matrix for {best_name}\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11629ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "    best_result = results[best_name]\n",
    "    \n",
    "    # Per-class F1 scores\n",
    "    per_class_df = pd.DataFrame({\n",
    "        'Organ': [ORGAN_CLASSES[i] for i in range(num_classes)],\n",
    "        'Region': [ORGAN_TO_REGION[ORGAN_CLASSES[i]] for i in range(num_classes)],\n",
    "        'F1-Score': best_result['per_class']['f1_score'],\n",
    "        'Support': best_result['per_class']['support']\n",
    "    })\n",
    "    per_class_df = per_class_df.sort_values('F1-Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nPer-Class Performance ({best_name}):\")\n",
    "    print(\"=\"*70)\n",
    "    print(per_class_df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    colors = [{'abdomen': '#FF6B6B', 'chest': '#4ECDC4', 'brain': '#45B7D1'}[r]\n",
    "              for r in per_class_df['Region']]\n",
    "    bars = ax.barh(per_class_df['Organ'], per_class_df['F1-Score'], color=colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Per-Class Performance - {best_name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim([0, 1.05])\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "               f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#FF6B6B', alpha=0.8, label='Abdomen'),\n",
    "        Patch(facecolor='#4ECDC4', alpha=0.8, label='Chest'),\n",
    "        Patch(facecolor='#45B7D1', alpha=0.8, label='Bone')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/per_class_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Saved to '../figures/per_class_performance.png'\")\n",
    "else:\n",
    "    print(\"No results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESEARCH FINDINGS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model\n",
    "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "    best_result = results[best_name]\n",
    "    best_info = model_info[best_name]\n",
    "    \n",
    "    print(f\"\\n BEST PERFORMING MODEL: {best_name}\")\n",
    "    print(f\"   Architecture: {best_info['architecture']}\")\n",
    "    print(f\"   Parameters: {best_info['params']/1e6:.2f}M\")\n",
    "    print(f\"   Test Accuracy: {best_result['accuracy']:.4f}\")\n",
    "    print(f\"   F1-Score: {best_result['f1_score']:.4f}\")\n",
    "    \n",
    "    # Most efficient\n",
    "    efficiency = {name: results[name]['accuracy'] / (model_info[name]['params'] / 1e6)\n",
    "                  for name in results.keys()}\n",
    "    most_efficient = max(efficiency.keys(), key=lambda k: efficiency[k])\n",
    "    \n",
    "    print(f\"\\n MOST EFFICIENT MODEL: {most_efficient}\")\n",
    "    print(f\"   Accuracy per Million Parameters: {efficiency[most_efficient]:.4f}\")\n",
    "    \n",
    "    # All models\n",
    "    print(f\"\\n ALL MODELS (sorted by accuracy):\")\n",
    "    for name in sorted(results.keys(), key=lambda k: results[k]['accuracy'], reverse=True):\n",
    "        acc = results[name]['accuracy']\n",
    "        params = model_info[name]['params'] / 1e6\n",
    "        print(f\"   {name:20s} - Acc: {acc:.4f}, Params: {params:6.2f}M\")\n",
    "    \n",
    "    print(\"\\n KEY FINDINGS:\")\n",
    "    print(\"   1. Evaluated multiple state-of-the-art 3D CNN architectures\")\n",
    "    print(\"   2. Analyzed accuracy vs efficiency trade-offs\")\n",
    "    print(\"   3. Identified optimal models for different use cases\")\n",
    "    \n",
    "    print(\"\\n RECOMMENDATIONS:\")\n",
    "    print(f\"   • For maximum accuracy: {best_name}\")\n",
    "    print(f\"   • For efficiency: {most_efficient}\")\n",
    "    print(\"   • For research: ResNet-18 provides good baseline\")\n",
    "    print(\"   • For deployment: Consider accuracy, speed, and memory\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All results saved to '../results/' and '../figures/'\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n No trained models available\")\n",
    "    print(\"   Run notebooks 02 and 03 first to train models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kfold_results_header",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation Results\n",
    "\n",
    "This section loads and visualizes the k-fold cross-validation results from notebook 03.\n",
    "These results provide a more robust estimate of model performance with uncertainty measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfold_results_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load k-fold results from notebook 03\n",
    "kfold_path = Path('../models/kfold_results.pth')\n",
    "\n",
    "if kfold_path.exists():\n",
    "    kfold_data = torch.load(kfold_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    coarse_results = kfold_data['coarse_kfold_results']\n",
    "    hierarchical_results = kfold_data['hierarchical_kfold_results']\n",
    "    k_folds = kfold_data['k_folds']\n",
    "    \n",
    "    print(f\"✓ Loaded k-fold results ({k_folds} folds)\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"\\n{'Model':<25} {'Accuracy':<20} {'AUC (macro)':<20} {'AUC (weighted)':<20}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    coarse_acc = np.array(coarse_results['fold_accuracies'])\n",
    "    coarse_auc_macro = np.array(coarse_results['fold_auc_macro'])\n",
    "    coarse_auc_weighted = np.array(coarse_results['fold_auc_weighted'])\n",
    "    \n",
    "    hier_acc = np.array(hierarchical_results['fold_accuracies'])\n",
    "    hier_auc_macro = np.array(hierarchical_results['fold_auc_macro'])\n",
    "    hier_auc_weighted = np.array(hierarchical_results['fold_auc_weighted'])\n",
    "    \n",
    "    print(f\"{'Coarse (Stage 1)':<25} {coarse_acc.mean():.4f} ± {coarse_acc.std():.4f}   {coarse_auc_macro.mean():.4f} ± {coarse_auc_macro.std():.4f}   {coarse_auc_weighted.mean():.4f} ± {coarse_auc_weighted.std():.4f}\")\n",
    "    print(f\"{'Hierarchical (Full)':<25} {hier_acc.mean():.4f} ± {hier_acc.std():.4f}   {hier_auc_macro.mean():.4f} ± {hier_auc_macro.std():.4f}   {hier_auc_weighted.mean():.4f} ± {hier_auc_weighted.std():.4f}\")\n",
    "    print(\"=\"*85)\n",
    "else:\n",
    "    print(\"K-fold results not found. Run notebook 03 first to generate k-fold results.\")\n",
    "    coarse_results = None\n",
    "    hierarchical_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfold_results_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k-fold results\n",
    "if kfold_path.exists() and coarse_results is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    folds = range(1, k_folds + 1)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(np.array(list(folds)) - 0.2, coarse_results['fold_accuracies'], 0.4, label='Coarse', color='steelblue', alpha=0.8)\n",
    "    axes[0].bar(np.array(list(folds)) + 0.2, hierarchical_results['fold_accuracies'], 0.4, label='Hierarchical', color='coral', alpha=0.8)\n",
    "    axes[0].axhline(y=np.mean(coarse_results['fold_accuracies']), color='steelblue', linestyle='--', alpha=0.6)\n",
    "    axes[0].axhline(y=np.mean(hierarchical_results['fold_accuracies']), color='coral', linestyle='--', alpha=0.6)\n",
    "    axes[0].set_xlabel('Fold')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Accuracy by Fold')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xticks(list(folds))\n",
    "    \n",
    "    # AUC Macro comparison\n",
    "    axes[1].bar(np.array(list(folds)) - 0.2, coarse_results['fold_auc_macro'], 0.4, label='Coarse', color='steelblue', alpha=0.8)\n",
    "    axes[1].bar(np.array(list(folds)) + 0.2, hierarchical_results['fold_auc_macro'], 0.4, label='Hierarchical', color='coral', alpha=0.8)\n",
    "    axes[1].axhline(y=np.mean(coarse_results['fold_auc_macro']), color='steelblue', linestyle='--', alpha=0.6)\n",
    "    axes[1].axhline(y=np.mean(hierarchical_results['fold_auc_macro']), color='coral', linestyle='--', alpha=0.6)\n",
    "    axes[1].set_xlabel('Fold')\n",
    "    axes[1].set_ylabel('AUC (Macro)')\n",
    "    axes[1].set_title('AUC (Macro) by Fold')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xticks(list(folds))\n",
    "    \n",
    "    # AUC Weighted comparison\n",
    "    axes[2].bar(np.array(list(folds)) - 0.2, coarse_results['fold_auc_weighted'], 0.4, label='Coarse', color='steelblue', alpha=0.8)\n",
    "    axes[2].bar(np.array(list(folds)) + 0.2, hierarchical_results['fold_auc_weighted'], 0.4, label='Hierarchical', color='coral', alpha=0.8)\n",
    "    axes[2].axhline(y=np.mean(coarse_results['fold_auc_weighted']), color='steelblue', linestyle='--', alpha=0.6)\n",
    "    axes[2].axhline(y=np.mean(hierarchical_results['fold_auc_weighted']), color='coral', linestyle='--', alpha=0.6)\n",
    "    axes[2].set_xlabel('Fold')\n",
    "    axes[2].set_ylabel('AUC (Weighted)')\n",
    "    axes[2].set_title('AUC (Weighted) by Fold')\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xticks(list(folds))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/kfold_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ K-fold comparison figure saved to '../figures/kfold_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfold_results_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-fold summary to CSV for easy reference\n",
    "if kfold_path.exists() and coarse_results is not None:\n",
    "    kfold_summary = pd.DataFrame([\n",
    "        {\n",
    "            'Model': 'Coarse (Stage 1)',\n",
    "            'Accuracy Mean': np.mean(coarse_results['fold_accuracies']),\n",
    "            'Accuracy Std': np.std(coarse_results['fold_accuracies']),\n",
    "            'AUC Macro Mean': np.mean(coarse_results['fold_auc_macro']),\n",
    "            'AUC Macro Std': np.std(coarse_results['fold_auc_macro']),\n",
    "            'AUC Weighted Mean': np.mean(coarse_results['fold_auc_weighted']),\n",
    "            'AUC Weighted Std': np.std(coarse_results['fold_auc_weighted']),\n",
    "        },\n",
    "        {\n",
    "            'Model': 'Hierarchical (Full)',\n",
    "            'Accuracy Mean': np.mean(hierarchical_results['fold_accuracies']),\n",
    "            'Accuracy Std': np.std(hierarchical_results['fold_accuracies']),\n",
    "            'AUC Macro Mean': np.mean(hierarchical_results['fold_auc_macro']),\n",
    "            'AUC Macro Std': np.std(hierarchical_results['fold_auc_macro']),\n",
    "            'AUC Weighted Mean': np.mean(hierarchical_results['fold_auc_weighted']),\n",
    "            'AUC Weighted Std': np.std(hierarchical_results['fold_auc_weighted']),\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    kfold_csv_path = '../results/kfold_summary.csv'\n",
    "    kfold_summary.to_csv(kfold_csv_path, index=False)\n",
    "    print(f\"\\nK-fold summary saved to '{kfold_csv_path}'\")\n",
    "    print(\"\\nK-Fold Summary:\")\n",
    "    print(kfold_summary.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
