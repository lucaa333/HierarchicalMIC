{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Results and Analysis\n",
                "\n",
                "## Hierarchical Classification: Architecture Comparison\n",
                "\n",
                "### Evaluation metrics: **Accuracy, Precision, Recall, F1-Score, AUC, ROC Curves**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, classification_report, accuracy_score,\n",
                "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
                ")\n",
                "from sklearn.preprocessing import label_binarize\n",
                "from tqdm import tqdm\n",
                "\n",
                "from config import (\n",
                "    DEVICE, DATA_CONFIG, MODEL_CONFIG, PATHS, set_seed, DEFAULT_MERGED_DATASETS\n",
                ")\n",
                "from utils.data_loader import create_hierarchical_dataset, REGION_FINE_CLASS_COUNTS\n",
                "from utils.hierarchical_model import HierarchicalClassificationModel\n",
                "\n",
                "set_seed(42)\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "train_loader, val_loader, test_loader, dataset_info = create_hierarchical_dataset(\n",
                "    datasets_to_include=DEFAULT_MERGED_DATASETS,\n",
                "    batch_size=DATA_CONFIG['batch_size'],\n",
                "    num_workers=DATA_CONFIG['num_workers']\n",
                ")\n",
                "\n",
                "print(f\"Test samples: {dataset_info['test_samples']}\")\n",
                "print(f\"Regions: {dataset_info['idx_to_region']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Discover Available Hierarchical Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ARCHITECTURES = ['enhanced', 'resnet18_3d', 'resnet34_3d', 'densenet121_3d', 'efficientnet3d_b0']\n",
                "\n",
                "hierarchical_models = {}\n",
                "\n",
                "print(\"Scanning for trained hierarchical models...\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for arch in ARCHITECTURES:\n",
                "    model_path = f\"{PATHS['models']}/hierarchical_{arch}.pth\"\n",
                "    if os.path.exists(model_path):\n",
                "        hierarchical_models[arch] = torch.load(model_path, map_location=DEVICE)\n",
                "        print(f\"Found: hierarchical_{arch}.pth\")\n",
                "\n",
                "print(\"-\" * 50)\n",
                "print(f\"Models found: {list(hierarchical_models.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load hierarchical model\n",
                "def compute_metrics(y_true, y_pred, y_proba=None, num_classes=None):\n",
                "    \"\"\"Compute accuracy, precision, recall, F1, and AUC.\"\"\"\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
                "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
                "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
                "    }\n",
                "    \n",
                "    # AUC (requires probability scores and multi-class handling)\n",
                "    if y_proba is not None and num_classes is not None and num_classes > 1:\n",
                "        try:\n",
                "            if num_classes == 2:\n",
                "                metrics['auc'] = roc_auc_score(y_true, y_proba[:, 1])\n",
                "            else:\n",
                "                y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
                "                metrics['auc'] = roc_auc_score(y_true_bin, y_proba, average='weighted', multi_class='ovr')\n",
                "        except Exception as e:\n",
                "            metrics['auc'] = None\n",
                "    else:\n",
                "        metrics['auc'] = None\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "# Compute metrics for each architecture\n",
                "all_metrics = {}\n",
                "\n",
                "for arch, checkpoint in hierarchical_models.items():\n",
                "    results = checkpoint['test_results']\n",
                "    \n",
                "    # Coarse (Stage 1) metrics\n",
                "    coarse_metrics = compute_metrics(\n",
                "        results['coarse_labels'],\n",
                "        results['coarse_predictions'],\n",
                "        num_classes=len(dataset_info['idx_to_region'])\n",
                "    )\n",
                "    \n",
                "    # Fine (Stage 2) metrics  \n",
                "    fine_metrics = compute_metrics(\n",
                "        results['fine_labels'],\n",
                "        results['fine_predictions']\n",
                "    )\n",
                "    \n",
                "    all_metrics[arch] = {\n",
                "        'coarse': coarse_metrics,\n",
                "        'fine': fine_metrics,\n",
                "        'per_region': results['fine_accuracy_per_region']\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"HIERARCHICAL MODEL COMPARISON - STAGE 1 (COARSE/REGION)\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(f\"\\n{'Architecture':<18} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'AUC':<12}\")\n",
                "print(\"-\" * 100)\n",
                "\n",
                "for arch in ARCHITECTURES:\n",
                "    if arch not in all_metrics:\n",
                "        continue\n",
                "    m = all_metrics[arch]['coarse']\n",
                "    auc_str = f\"{m['auc']:.4f}\" if m['auc'] else \"N/A\"\n",
                "    print(f\"{arch:<18} {m['accuracy']:<12.4f} {m['precision']:<12.4f} {m['recall']:<12.4f} {m['f1']:<12.4f} {auc_str:<12}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"HIERARCHICAL MODEL COMPARISON - STAGE 2 (FINE/PATHOLOGY)\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(f\"\\n{'Architecture':<18} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
                "print(\"-\" * 100)\n",
                "\n",
                "for arch in ARCHITECTURES:\n",
                "    if arch not in all_metrics:\n",
                "        continue\n",
                "    m = all_metrics[arch]['fine']\n",
                "    print(f\"{arch:<18} {m['accuracy']:<12.4f} {m['precision']:<12.4f} {m['recall']:<12.4f} {m['f1']:<12.4f}\")\n",
                "\n",
                "print(\"-\" * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"FINE ACCURACY PER ANATOMICAL REGION\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(f\"\\n{'Architecture':<18} {'Abdomen':<15} {'Chest':<15} {'Brain':<15}\")\n",
                "print(\"-\" * 100)\n",
                "\n",
                "for arch in ARCHITECTURES:\n",
                "    if arch not in all_metrics:\n",
                "        continue\n",
                "    pr = all_metrics[arch]['per_region']\n",
                "    print(f\"{arch:<18} {pr.get('abdomen', 0):<15.4f} {pr.get('chest', 0):<15.4f} {pr.get('brain', 0):<15.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Curves Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if all_metrics:\n",
                "    archs = list(all_metrics.keys())\n",
                "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "    \n",
                "    x = np.arange(len(archs))\n",
                "    width = 0.2\n",
                "    \n",
                "    # Stage 1 (Coarse) metrics\n",
                "    for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
                "        values = [all_metrics[a]['coarse'][metric] for a in archs]\n",
                "        axes[0].bar(x + i*width, values, width, label=metrics_names[i])\n",
                "    \n",
                "    axes[0].set_xlabel('Architecture')\n",
                "    axes[0].set_ylabel('Score')\n",
                "    axes[0].set_title('Stage 1 (Coarse) - Region Classification')\n",
                "    axes[0].set_xticks(x + width*1.5)\n",
                "    axes[0].set_xticklabels(archs, rotation=45, ha='right')\n",
                "    axes[0].legend()\n",
                "    axes[0].set_ylim(0, 1)\n",
                "    axes[0].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    # Stage 2 (Fine) metrics\n",
                "    for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
                "        values = [all_metrics[a]['fine'][metric] for a in archs]\n",
                "        axes[1].bar(x + i*width, values, width, label=metrics_names[i])\n",
                "    \n",
                "    axes[1].set_xlabel('Architecture')\n",
                "    axes[1].set_ylabel('Score')\n",
                "    axes[1].set_title('Stage 2 (Fine) - Pathology Classification')\n",
                "    axes[1].set_xticks(x + width*1.5)\n",
                "    axes[1].set_xticklabels(archs, rotation=45, ha='right')\n",
                "    axes[1].legend()\n",
                "    axes[1].set_ylim(0, 1)\n",
                "    axes[1].grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f\"{PATHS['figures']}/metrics_comparison.png\", dpi=150)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_roc_curves(y_true, y_pred, class_names, title, save_path):\n",
                "    \"\"\"Plot ROC curves for multi-class classification.\"\"\"\n",
                "    n_classes = len(class_names)\n",
                "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
                "    y_pred_bin = label_binarize(y_pred, classes=range(n_classes))\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    colors = plt.cm.Set1(np.linspace(0, 1, n_classes))\n",
                "    \n",
                "    for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
                "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        ax.plot(fpr, tpr, color=color, lw=2, label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
                "    \n",
                "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
                "    ax.set_xlim([0.0, 1.0])\n",
                "    ax.set_ylim([0.0, 1.05])\n",
                "    ax.set_xlabel('False Positive Rate')\n",
                "    ax.set_ylabel('True Positive Rate')\n",
                "    ax.set_title(title)\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(save_path, dpi=150)\n",
                "    plt.show()\n",
                "\n",
                "if all_metrics:\n",
                "    # Find best model\n",
                "    best_arch = max(all_metrics.keys(), key=lambda a: all_metrics[a]['fine']['f1'])\n",
                "    results = hierarchical_models[best_arch]['test_results']\n",
                "    \n",
                "    print(f\"\\n ROC Curves for Best Model: {best_arch}\")\n",
                "    \n",
                "    # Coarse ROC\n",
                "    region_labels = [dataset_info['idx_to_region'][i] for i in range(len(dataset_info['idx_to_region']))]\n",
                "    plot_roc_curves(\n",
                "        results['coarse_labels'],\n",
                "        results['coarse_predictions'],\n",
                "        region_labels,\n",
                "        f'ROC Curves - Stage 1 Coarse ({best_arch})',\n",
                "        f\"{PATHS['figures']}/roc_coarse_{best_arch}.png\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_confusion_matrix(y_true, y_pred, title, labels=None, save_path=None):\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
                "                xticklabels=labels, yticklabels=labels)\n",
                "    ax.set_xlabel('Predicted')\n",
                "    ax.set_ylabel('True')\n",
                "    ax.set_title(title)\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150)\n",
                "    plt.show()\n",
                "\n",
                "if all_metrics:\n",
                "    best_arch = max(all_metrics.keys(), key=lambda a: all_metrics[a]['fine']['f1'])\n",
                "    results = hierarchical_models[best_arch]['test_results']\n",
                "    \n",
                "    # Coarse confusion matrix\n",
                "    region_labels = [dataset_info['idx_to_region'][i] for i in range(len(dataset_info['idx_to_region']))]\n",
                "    plot_confusion_matrix(\n",
                "        results['coarse_labels'],\n",
                "        results['coarse_predictions'],\n",
                "        f'Confusion Matrix - Stage 1 ({best_arch})',\n",
                "        labels=region_labels,\n",
                "        save_path=f\"{PATHS['figures']}/confusion_coarse_{best_arch}.png\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"FINAL RANKING BY F1-SCORE\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "sorted_archs = sorted(all_metrics.keys(), key=lambda a: all_metrics[a]['fine']['f1'], reverse=True)\n",
                "\n",
                "print(f\"\\n{'Rank':<6} {'Architecture':<18} {'Coarse F1':<12} {'Fine F1':<12} {'Fine Acc':<12}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for rank, arch in enumerate(sorted_archs, 1):\n",
                "    m_c = all_metrics[arch]['coarse']\n",
                "    m_f = all_metrics[arch]['fine']\n",
                "    print(f\"{rank:<6} {arch:<18} {m_c['f1']:<12.4f} {m_f['f1']:<12.4f} {m_f['accuracy']:<12.4f}\")\n",
                "\n",
                "print(\"-\" * 60)\n",
                "\n",
                "if sorted_archs:\n",
                "    best = sorted_archs[0]\n",
                "    m = all_metrics[best]\n",
                "    print(f\"\\n BEST MODEL: {best}\")\n",
                "    print(f\"   Stage 1 - Accuracy: {m['coarse']['accuracy']:.4f}, F1: {m['coarse']['f1']:.4f}\")\n",
                "    print(f\"   Stage 2 - Accuracy: {m['fine']['accuracy']:.4f}, F1: {m['fine']['f1']:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
